<font size=6 color=Purple>**困于心，衡于虑，而后作；征于色，发于声，而后喻。**</font>
# 第六部分 Flow-Based Model
#### 参考内容
https://jmtomczak.github.io/blog/3/3_flows.html  
https://blog.csdn.net/a312863063/article/details/94306107  
https://blog.csdn.net/shaoyue1234/article/details/102464867  
NICE: NON-LINEAR INDEPENDENT COMPONENTS ESTIMATION

## 6.1 概率基础
上一部分总结的自回归模型是通过乘法法则直接求出 $P(X)$ ，那么有没有别的直接建模 $P(X)$ 的方法呢？回顾一下概率论基础的部分，变量替换公式也可以达到这个效果。
设 $f:\Bbb{R}^D \to \Bbb{R}^D$ 为可逆变换， $x = f(z), z = f^{-1}(x)$，$X$ 的概率密度为 $p_x(x)$`，`$z$ 的概率密度为 $p_z(z)$ 。根据变量替换公式：
$$
p_x(x) = p_z(z = f^{-1}(x))|\frac{\partial f^{-1}(x)}{\partial x}|
$$
其中
$$
|\frac{\partial f^{-1}(x)}{\partial x}| = |det \textbf{ J}_{f^{-1}}(x)| = |det \textbf{ J}_{f}(x)|^{-1}
$$
即Jacobian矩阵的行列式的绝对值。
于是可得：
$$
p_x(x) = p_z(z = f^{-1}(x))|det \textbf{ J}_{f}(x)|^{-1}
$$

## 6.2 Flow-Based Model
流模型（Flow-Based Model）采用的思想正是变量替换公式。通过一系列的可逆变换 $f_k:\Bbb{R}^D \to \Bbb{R}^D$ ，先将未知的分布 $p_x(x)$ 变换到一个已知的分布 $p_{z_0}(z_0)$ 上,比如可以假设 $z \thicksim N(z | 0, I)$ 即标准正态分布。那么通过逆变换就可以将 $p_z(z)$ 变换回 $p_x(x)$ ：
$$
\begin{aligned}
p_x(x) &= p_{z_0}(z_0 = f^{-1}(x)) \prod_{i=1}^K |det \frac{\partial f_i(z_{i-1})}{\partial z_{i-1}}|^{-1} \\
&= p_{z_0}(z_0 = f^{-1}(x)) \prod_{i=1}^K | \textbf{J}_{f_i}(z_{i-1})|^{-1}
\end{aligned}
$$
![img](res/08/001.png)

## 6.3 可逆函数设计
为了设计可逆的神经网络，而且这个可逆变换的Jacobian矩阵需要容易计算，Flow模型设计了特殊的结构。
### 6.3.1 Coupling Layer：
将输入的 $D$ 维向量，进行拆分即 $x = [x_a, x_b] = [x_{1:d}, x_{d+1:D}]$ ，那么coupling layer要做的就是如下转换：
$$
\begin{aligned}
y_a &= x_a \\
y_b &= exp(s(x_a)) \odot x_b + t(x_a)
\end{aligned}
$$
其中 $s(\cdot)$ 和 $t(\cdot)$ 可以是任意的神经网络，称作scaling和transition。这种coupling layer的变换是可逆的，
$$
\begin{aligned}
x_a &= y_a \\
x_b &= (y_b - t(y_a)) \odot exp(-s(y_a))
\end{aligned}
$$
那么这个转换的Jacobian矩阵为：
$$
\begin{aligned}
J &=
\begin{bmatrix}
I_{d \times d} & 0_{d \times (D-d)} \\
\frac{\partial y_b}{\partial x_a} & diag(exp(s(x_a)))
\end{bmatrix} \\
det(J) &= \prod_{j-1}^{D-d} exp(s(x_a))_j \\
&= exp(\sum_{j-1}^{D-d} s(x_a)_j)
\end{aligned}
$$

#### Additive Coupling Layer
举个例子，一种比较特殊的Coupling Layer，叫Additive Coupling Layer，
$$
\begin{aligned}
y_a &= x_a \\
y_b &= x_b + m(x_a)
\end{aligned}
$$
相应的逆变换就是
$$
\begin{aligned}
x_a &= y_a \\
x_b &= y_b - t(y_a)
\end{aligned}
$$
这种变换的特殊之处在于它的Jacobian矩阵
$$
\begin{aligned}
J &=
\begin{bmatrix}
I_{d \times d} & 0_{d \times (D-d)} \\
\frac{\partial y_b}{\partial x_a} & I_{(D-d) \times (D-d)})
\end{bmatrix} \\
det(J) &= 1
\end{aligned}
$$

### 6.3.2 Scaling Layer
对于Additive Coupling Layer这种，Jaccobian矩阵的行列式为1，如果想对它加入放缩变换怎么做，很简单乘上一个对角矩阵 $S$ ，这也就是Scaling Layer。


## 6.4 目标函数
那么对于Flow模型，依然用极大似然来推导其优化的目标
$$
\begin{aligned}
\underset{\theta}{\operatorname{argmax}} P(\mathscr{D} | \theta) &= \underset{\theta}{\operatorname{argmax}} \sum_{n} log(P(\pmb{x_n} | \theta)) + Regularization \\
log(P(x | \theta)) &= log(P_z(z = f^{-1}(x)) \prod_{i=1}^K | \textbf{J}_{f_i}(z_{i-1})|^{-1})) \\
&= log(P_z(z = f^{-1}(x))) - \sum_{i=1}^K(log(| \textbf{J}_{f_i}(z_{i-1})|)) \\
\end{aligned}
$$
如果取 $z \thicksim N(0, I)$ ，那么目标的前半部分：
$$
log(P_z(z = f^{-1}(x))) = -\frac{D}{2} log(2 \pi) - \frac{1}{2} ||f^{-1}(x)||^2
$$
一目了然，这个前半部分就是在约束从 $x$ 经过转换后的 $z$ 的分布要与标准正态分布尽可能的相同。

那么后半部分，根据Jacobian矩阵行列式的意义，就是在最大化 $z$ 的分布空间到 $x$ 分布空间的变化率。
怎么理解，比如现在假设两个随机变量， $x \thicksim Unif(0,1)$ 服从0到1的均匀分布， $z \thicksim Unif(0,2)$ 服从0到2的均匀分布，那么变换 $z = 2x$ 就能完成两个分布的变换。此时 $|det J_f| = 2$ 就是变化率，因为是把小范围0-1的随机变量，扩到了0-2上，范围在扩大。
那么对于Flow来说, $z \thicksim N(0, I)$ 是 $D$ 维的标准正态分布，那么就有 $D$ 个自由度，但是我们的训练数据的 $D$ 维之间是互相有关联的，自由度要小于 $D$ 。也就是说，我们训练数据的分布空间的范围要比标准正态分布的空间要小，那么就需要增大这个变化率，好让训练数据空间经过可逆映射能够覆盖整个标准正态分布的空间。因为我们最后在利用Flow采样生成新的样本的时候，是要从整个标准正态分布中随机采样的。

## 6.5 一些补充观点
这里介绍的流模型称作离散流模型，之后会介绍连续流模型。
另外，Flow模型的目标也可以看做是在做变分推断。在这种观点下，x经过K个可逆变换得到的z的分布可以表示为 $P(z|x)$ ，但是要得到精确分布并不容易，于是采用变分推断的思想，用一个已知的分布 $P(z)$ ，比如标准正态分布，来逼近后验分布。在这种观点下，Flow的优化目标的前半部分就是在优化先验分布与后验分布之间的KL散度，也就是在最大化变分下界（ELBO）。其实这种变分的思想适用于很多其他模型，比如VAE等。
